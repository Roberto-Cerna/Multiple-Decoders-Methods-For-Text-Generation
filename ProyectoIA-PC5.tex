%\documentclass[11pt,twocolumn,spanish]{article} 
\documentclass[10pt,twocolumn]{article}
\usepackage[spanish,english]{babel}
\usepackage{indentfirst}
\usepackage{anysize} % Soporte para el comando \marginsize
%\marginsize{1.5cm}{1.5cm}{0.5cm}{1cm}
\marginsize{2,5cm}{1,8cm}{1.5cm}{1,7cm}
\usepackage[psamsfonts]{amssymb}
\usepackage{float}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multicol}
\usepackage{multirow} 
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tocloft}
\usepackage{natbib}
\usepackage[spanish,es-tabla]{babel}
\usepackage[utf8]{inputenc}
\usepackage{subcaption}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\renewcommand*\contentsname{Summary}
\renewcommand{\thepage}{}
\theoremstyle{definition}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\begin{document}
	
\begin{center}
	%\textbf{Curso:}\\
	\vspace{5pt}
	{\large \textbf{COMO GENERAR TEXTO, UTILIZANDO DIFERENTES MÉTODOS DE DECODIFICACIÓN, PARA LA GENERACIÓN DE LENGUAJES CON TRANSFORMADORES}}\\
	%{\large \textbf{Laboratorio 1} }\\
\end{center}

\begin{center}
	Students:\\
	\vspace{5pt}
	Universidad Nacional de Ingeniería\\
	\vspace{5pt}
	{\large Roberto Alexis Cerna Espiritu }\\
	e-mail: roberto.cerna.e@uni.pe\\
	{\large Abel Alejandro Oliva Valdivia }\\
	e-mail: abel.oliva.v@uni.pe\\
	{\large Franz Rony Ventocilla Tamara }\\
	e-mail: fventocillat@uni.pe\\
	{\large Jesús Miguel Yacolca Huamán }\\
	e-mail: jyacolcah@uni.pe\\
	{\large Eros Aylthon Vargas Torres }\\
	e-mail: evargast@uni.pe\\
	
\end{center}
\vspace{5pt}
%\begin{center}
%	Curso:\\
%	\vspace{5pt}
%	{\large CC0A2 Programación de Dispositivos Móviles}\\
%	{\large Laboratorio 1}\\
%\end{center}
\vspace{20pt}
\begin{abstract*}
{\small
\hspace*{0.5cm}

\begin{center}
    \textbf{Resumen}
\end{center}

\\
La codificación es el proceso mediante el cual la información se convierte en otra forma aceptable para la transmisión. La decodificación invierte este proceso para interpretar la información. En el siguiente trabajo de investigación se realiza un análisis sobre las obtención del texto, basado en la en cinco métodos de decodificación. En primer lugar, se analizaran los datos y se explicará brevemente el funcionamiento de los métodos que usaremos para posteriormente obtener el mensaje codificado.

\textbf{Palabras Clave:} Codificación, Decodificación.
}

\end{abstract*}

\begin{abstract*}
{\small
\hspace*{0.5cm}

\begin{center}
    \textbf{Abstract}
\end{center}

\\
Encryption is the process by which information is converted into another acceptable form for transmission. Decoding reverses this process to interpret the information. In the following research work, an analysis is carried out on obtaining the text, based on the five decoding methods. In the first place, the data will be analyzed and the operation of the methods that we will use to later obtain the encoded message will be briefly explained.

\textbf{Keywords:} Encoding, Decoding.
}

\end{abstract*}

\pagenumbering{arabic}

\tableofcontents

\vspace{20pt}
\hrule
\vspace{10pt}

%\newpage      comienzo     \section{Resumen}


\section{Introducción}

\subsection{Planteamiento del problema}
El hacer que una maquina pueda ser capaz de interpretar lo que le dicen o escriben, ha sido un problema de la comprensión automático, pero a partir de 1960 surgen los primero Procesamientos de lenguaje natural los cuales han ido mejorando con el pasar del tiempo.  El problema de los últimos años es saber cuál de los últimos métodos utilizados para decodificación es el más óptimo, por lo que usaremos varios métodos para hacer pruebas, además de se evaluará y comparará la calidad del texto que producen, para ello usaremos la librería torch, también usaremos un modelador de lenguaje en español basado del conocido generador de texto GPT-2, en adición se utilizará una API de transformers realizada por Hugging Face.

\subsubsection{Torch}
Es una biblioteca de código abierto para aprendizaje automático, un marco de computación científica, y un lenguaje de script basado en el lenguaje de programación Lua(lenguaje multiparadigma, imperativo y estructurado).Proporciona una amplia gama de algoritmos de aprendizaje profundo, y usa el lenguaje de script LuaJIT, sobre una implementación en C.

\subsubsection{GPT-2}
Es tan sólo el último hito en el campo del PNL (procesamiento de lenguaje natural), es una inteligencia artificial de código abierto creada por OpenAI en febrero de 2019. GPT-2 traduce texto, responde preguntas, resume pasajes, y genera una salida de texto en un nivel que, aunque a veces no se puede distinguir del de los humanos,  puede volverse repetitivo o sin sentido al generar pasajes largos.

\subsubsection{Hugging Face}
Es una API la cual podría detectar emociones y responder preguntas según el contexto y las emociones. Hugging Face tiene como objetivo convertirse en GitHub para el aprendizaje automático, es además una de las startups líderes en el espacio de la PNL.

\subsection{Objetivos}
% para otras secciones .....\subsubsection{Presentación2}
% para que tenga items \begin{itemize} ---  \end{itemize}
\begin{itemize}
    \item Poner a prueba algunos métodos de decodificación y observar su funcionamiento.
    \item Implementar con la ayuda de la librería torch, los métodos de decodificación elegidos.
    \item Comprender el funcionamiento de los diferentes métodos y evaluar cuál de ellos procesan una mejor calidad de texto.   

\end{itemize}


\subsection{Organización del informe}
falta

\subsection{Estado del arte}
falta

\subsection{Aporte de los artículos }
falta

\subsection{Mención de Articulos}
falta

\subsection{Metodología}
falta


\section{Diseño del experimento}

%% para poner en negro     \textbf{¿Pero no íbamos a usar papel?} \\
\subsection{Método : Búsqueda Greedy}
También llamado búsqueda Voraz, es una estrategia de búsqueda por la cual se sigue una heurística consistente en elegir la opción óptima en cada paso local con la esperanza de llegar a una solución general óptima.

\subsection{Método : Beam Search}
También conocido como la estrategia de corte de caminos, mantiene un número predeterminado de las mejores rutas de búsqueda encontradas hasta el momento en un punto dado. Por lo tanto, considera más posibilidades que la primera profundidad de la búsqueda, pero evita el número exponencial de posibilidades de la primera extensión de búsqueda.
Realiza una búsqueda en extensión e incorpora una heurística para escoger en cada nivel solo los mejores nodos. Este método sacrifica completitud a cambio de un enfoque heurístico muy efectivo. Para simplificar, se escogerá múltiples secuencias, siempre tratando de buscar las secuencias con mayor probabilidad y manteniendo el mismo número de secuencias.

\subsection{Método : Sampling}
Este método de muestreo probabilístico sugiere seleccionar datos individuales o de un subconjunto y elegir la palabra que menor probabilidad tenga, esto genera que nuestra elección ya no sea determinista.

\subsection{Método : Top-k Sampling}
El top-k se define como los k elementos más frecuentes dentro de un conjunto de datos y su determinación, este método lo que hace es filtrar las  k palabras más probables y redistribuir las probabilidades entre esas k palabras.

\subsection{Método : Top-P Sampling}
El top-p se define como la probabilidad dentro de un conjunto de datos, en este caso el método va a definir una probabilidad p, entonces se escogerá el subconjunto de menor tamaño de palabras de tal manera que su probabilidad acumulada sea mayor a p.

\section{Experimentos y resultados}
falta

\section{Discusiones}
\begin{itemize}
    \item Respecto a la búsqueda Greedy, en el método tokenizer.decode transforma un tensor de ids al vocabulario (texto entendible para las personas), como parámetros opcionales especificamos obviar caracteres especiales como classifiers o separators.
    \item Para el método Beam search, el texto no es tan espontáneo ni natural como el de los humanos, para ello se introducirá aleatoriedad.
    \item En el método Sampling existen incoherencias en el significado, para ello utilizaremos otro método para filtrar que palabras se deben escoger.  
    \item Para el ejemplo tratado con Top-p Sampling, si queremos minizimar el número de palabras y maximizar la probabilidad acumulada iremos escogiendo las palabras con mayor probabilidad hasta llegar a superar la probabilidad p.
    
\end{itemize}

\section{Conclusiones}
\begin{itemize}
    \item Usando el método de búsqueda Greedy,notamos que caemos fácilmente en repeticiones lo cual no nos resulta tan útil.
    \item Al usar el método Beam search podemos observar que hemos mejorado un poco con respecto a greedy Search pero aún el texto no es tan espontáneo ni natural como el de los humanos, 
    \item Al usar el método Sampling se puede ver que el texto es un poco más impredecible, pero existen incoherencias en el significado.
    \item Usando el método Top-k Sampling notamos que el texto es mucho más humano, sin embargo no parece ser la mejor técnica, aun así produce un buen resultado.
    \item Utilizando el método Top-p Sampling se puede generar texto con un nivel de coherencia aceptable, además notamos que a diferencia de las secuencias generadas en beam search los resultados son diferentes y más impredecibles.
    \item Entre las cinco pruebas hechas podemos concluir que los mejores métodos a utilizar serian Top-k Sampling y Top-p Sampling

\end{itemize}

\newpage
\section{Bibliografía y referencias}

\begin{itemize}
    \item \url{https://www.bbva.com/es/criptomonedas-sirven-las-monedas-virtuales/}
    \item \url{https://scielo.conicyt.cl/scielo.php?script=sci_arttext&pid=S0719-25842019000100029}
    \item \url{https://criptomoneda.ninja/bitcoin/}
    \item \url{https://www.bbc.com/mundo/noticias-57066481}
    \item \url{https://especiales.dinero.com/bitcoin/index.html}
    \item \url{https://en.wikipedia.org/wiki/Bitcoin}
\end{itemize}
\end{document}

%para imagenes 
%       \begin{figure}[H]
%       \centering
%       \includegraphics[scale=0.95]{3.png}
%       %\caption{Vista activity-main.xml}
%       \end{figure}


